#+TITLE: Matrix Computations with Apache Spark
#+AUTHOR: Gustaf Waldemarson

* Introduction

  In this report the framework for performing linear algebra in the cloud using
  the Apache Spark (CITE) library LINALG (CITE). In particular the matrix
  methods /Singular Value Decomposition/ (SVD) and /QR Decomposition/ from this
  library are evaluated. The performance of these methods are evaluated on a
  gradually increasing cluster size using the /free/ tier on the Google DataProc
  platform. (CITE).

  # In particular, the performance received is compared against that of a local
  # machine, to evaluate if the cloud can provide any beneficial speed-up using
  # the /free/ tier on the Google DataProc platform (CITE).

* Terminology

  - Sparse  :: A matrix such that the quotient between the number of zero and
               non-zero elements is less than $0.5$
  - Dense   :: A matrix with more non-zero elements than zero elements.
  # - (S)GEMM :: (Single Precision floating point) General Matrix Multiplication.
  # - $O(n)$  :: Big-O-notation for approximating the complexity of algorithms by
  #              removing all factors but the most dominating one.

* Theory

** Singular Value Decomposition

   The idea behind

   For more details regarding the decomposition, see for instance (CITE).

** QR Decomposition

   For more details regarding the decomposition, see for instance (CITE).


* Approach

  # To evaluate the performance of matrix multiplication running on cloud
  # computers the results will be compared with those of a local machine (A Lenovo
  # T480s laptop, with an Intel(R) Core(TM) i7-8650U CPU @ 1.90GHz).

  To evaluate the cloud environment, clusters are created as follows:

  - Using single core workers with 3.5 GB of memory, the run-time and memory
    usage is measured for clusters of size 3 to 8 (including master node).

  - Using dual core workers with 7.5 GB of memory, the run-time and memory usage
    is measured for clusters of size 3 to 5 (including master node).

  On each of these configuration, the $5000x5000$ matrix /add32/ (CITE) is used
  for the SVD decomposition and the $1000x1000$ matrix /G45/ (CITE) is used for
  the QR decomposition. Each test is repeated at least 3 times to estimate the
  average run-time and memory use.

  For the SVD, we only compute the first 100 singular values, in order to reduce
  the necessary amount of memory.

  The run-time and memory usage is estimated from the /Elapsed time/ and /Yarn
  MEMORY/ in Google Cloud Console (figs (CITE)).

  Note that the matrix is preprocessed to a serialized sparse row-matrix format
  in order to reduce the impact of parsing the MatrixMarket format.

  Note that as we are using of-the-shelf algorithms, we assume that the result
  is always correct and thus only concern ourselves with the execution
  statistics.

* Results

** QR Decomposition

*** Single Core Clusters

    #+TBLNAME: qr-rt-sc-table
    | vCPUs |     1 |     2 |     3 |  Mean | Unit |
    |-------+-------+-------+-------+-------+------|
    |     3 |  1:27 |  1:05 | 00:59 | 01:10 |      |
    |     4 | 00:50 | 00:54 | 00:50 | 00:51 |      |
    |     5 | 01:17 | 00:55 | 00:54 | 01:02 |      |
    |     6 | 01:18 | 00:54 | 00:54 | 01:02 |      |
    |     7 |       |       |       | 00:00 |      |
    |     8 |       |       |       | 00:00 |      |
    #+TBLFM: $5=vmean($2..$4);UE

    #+TBLNAME: qr-mem-table
    | vCPUs      |    3 |    4 |    5 |     6 | 7 | 8 | Unit |
    |------------+------+------+------+-------+---+---+------|
    | Memory use | 3.95 | 6.92 | 9.87 | 12.88 |   |   | GB   |

    #+BEGIN_SRC gnuplot :var data=qr-rt-sc-table :exports results :file qr-sc.png
reset
set title "QR Decomposition Performance"
set xlabel "Cluster Size"
set xtics 1,1,8
set ylabel "Time (mm:ss)"
set ydata time
set timefmt "%M:%S"
plot data using 1:5 with lp lw 2 notitle
    #+END_SRC

    #+RESULTS:
    [[file:qr-sc.png]]


*** Dual Core Clusters


    | vCPUs      | 6 | 8 |
    |------------+---+---|
    | Memory use |   |   |


* Results

** Singular Value Decomposition

*** Single Core Clusters

    #+TBLNAME: svd-rt-sc-table
    | vCPUs |    1 |    2 |    3 |  mean | Unit  |
    |-------+------+------+------+-------+-------|
    |     3 | 6:35 | 6:30 | 6:26 | 06:30 | mm:ss |
    |     4 | 4:00 | 3:55 | 3:55 | 03:56 |       |
    |     5 | 4:05 | 4:00 | 4:04 | 04:03 |       |
    |     6 | 4:04 | 4:19 | 4:04 | 04:09 |       |
    |     7 |      |      |      | 00:00 |       |
    |     8 |      |      |      | 00:00 |       |
    #+TBLFM: $5=vmean($2..$4);UE

    #+TBLNAME: svd-mem-sc-table
    | vCPUs      |    3 |    4 |    5 |    6 | 7 | 8 | Unit |
    |------------+------+------+------+------+---+---+------|
    | Memory use | 3.95 | 6.92 | 9.87 | 0.98 |   |   | GB   |

    #+BEGIN_SRC gnuplot :var data=svd-rt-sc-table :exports results :file svd-sc.png
reset
set title "SVD Performance"
set xlabel "Cluster Size"
set xtics 1,1,8
set ylabel "Time (mm:ss)"
set ydata time
set timefmt "%M:%S"
plot data using 1:5 with lp lw 2 notitle
    #+END_SRC

    #+RESULTS:
    [[file:svd-sc.png]]

*** Dual Core Clusters

    | vCPUs         | 6 | 8 |
    |---------------+---+---|
    | Run-time 1    |   |   |
    | Run-time 2    |   |   |
    | Run-time 3    |   |   |
    | Run-time mean |   |   |

    | vCPUs           | 6 | 8 |
    |-----------------+---+---|
    | Memory use 1    |   |   |
    | Memory use 2    |   |   |
    | Memory use 3    |   |   |
    | Memory use mean |   |   |


* Further Work

  Due to the limited scope of this project, a large amount of interesting topics
  in computational linear algebra is by necessity omitted.

  # To begin, it would be very interesting to see how matrix multiplication
  # perform on GPUs and by extension, on GPUs in the cloud. This is a significant
  # topic in itself since performance can vary greatly depending on the
  # vendors. (NEEDS CITE).

  This project only focused on square matrices to keep the project simple, a
  bigger project should also evaluate performance on non-square matrices.

  Also, the experiments mostly focused on sparse matrices, as that was the only
  type of matrices easily obtained from the SuiteSparse Matrix collection
  (CITE). A larger study should compare larger dense matrices as well as sparse
  ones.

  Additionally, there are at several other classes of matrices that could be
  evaluated:

  - Diagonal  :: Matrix with elements exclusively on the central diagonal of the
                 matrix.
  - Banded    :: Matrix with elements along some diagonals of the matrix.
  - Symmetric :: A matrix whose transposition is equal to itself.

  Since there are often specialized routines for these kinds of matrices, a
  project of larger scope should incorporate evaluations of these types of
  matrices.

  Additonally, this project relied rather heavily on manually entering the
  performance data, but a larger project should extract this data
  programatically via the available monitoring APIs.


* Conclusions

  As can be seen in the results, scaling up the cluster can greatly increase the
  speed of some of these linear algebra computations. However, it is worthwhile
  to keep in mind that scaling up the cluster also increases the cost of running
  the cluster so it seems like it is a good idea to scale the cluster to an
  appropriate size in relation to the intended input data.

* GitHub Sources

  The code used for this project is available here:
  [[https://github.com/Xaldew/wasp-cc]]. It is structured roughly as follows:

  - ~src/~       :: Contains the /main/ collection of source code.
  - ~report/~    :: Contains the source for this report.
  - ~scripts/~   :: Contains various support scripts for working with the cloud.
  - ~tutorials/~ :: Contains test scripts.
  - ~data/~      :: Contains test data.

* Comments on the Assignment

  In general, the assignment was not particularly hard, but the lack of
  directions made it a bit unclear what results were actually desired. Also, the
  general lack of good examples for running Spark on Google Cloud made this
  assignment far more vexing than necessary.

  To begin with the Google Cloud platform is massive and contains many confusing
  and often contradictory terms; and to make matters worse: Doing something
  wrong can result in a significant loss of real money.

  # Thus, simply pointing us to the front page of Google DataProc and telling us
  # to "get crackin'" is absolutely the wrong approach for this kind of project.

  # Also, what is exactly is "Matrix computations"? While I'm all for open-ended
  # questions and open investigations in research topics, this is unnecessarily
  # vague. Either be frank to the students and ask them to choose one kind of
  # matrix computation to study or explicitly state what computations one should
  # investigate.

  Moreover, the instructions strongly suggest using preemptible instances,
  however, when you dig into the documentations for the "free" instances you
  notice that you are required to use 1 master and at least 2 nodes *normal*
  worker nodes. And, since you are limited 8 vCPUs in *total* it is not
  worthwhile to try to fit preemptible instances into this quota. For a task
  like this it just adds complexity and should simply be omitted.

  Additionally, all given examples are mostly a collection of examples of how to
  perform *functional programming* and not cloud computing. A lot of details
  regarding how to host/transfer large datasets is completely missing. This is
  difficult part of this assignment, not how to compose functions.

  # Regarding the SuiteSparse Matrix collection: This struck me as rather odd
  # choice of dataset-source since it almost exclusively contains sparse matrices,
  # meaning that the type of matrices and computations we can investigate is
  # drastically limited.

  # Additionally, for larger matrices it became increasingly difficult to find two
  # different matrices of the same dimensions, which would force us the either
  # truncate some other matrix and make notes of that, or only perform
  # matrix-squaring instead of multiplying.

  # To make matters worse, the behavior of matrix-multiplication differs
  # significantly depending on which kind of matrices one is operating on, meaning
  # that final results are extremely dependent on the user choices. This makes it
  # very hard to design a proper experiment and draw reasonable conclusions from
  # the results.
